{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/Xenia/PycharmProjects/SystematicRepresentations/')\n",
    "from egg.zoo.systematicity.metrics.tre import Objective, LinearComposition\n",
    "from egg.zoo.systematicity.metrics.tre import CompositionFunction, MultipleCrossEntropyLoss\n",
    "from typing import Iterable, Type\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from abc import ABC\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protocol(interaction, vocab_size):\n",
    "\n",
    "    sender_in = interaction.sender_input.cpu()\n",
    "    n_atts = int(sum(sender_in[0]))\n",
    "    n_vals = int(len(sender_in[0]) // n_atts)\n",
    "    messages = interaction.message[:, :-1].cpu() - 1\n",
    "\n",
    "    k_hot_messages = []\n",
    "    for m in messages:\n",
    "        k_hot_messages.append(torch.nn.functional.one_hot(\n",
    "            m, num_classes=vocab_size).reshape(-1))\n",
    "    k_hot_messages = torch.stack(k_hot_messages, dim=0)\n",
    "\n",
    "    derivations = []\n",
    "    for att in range(n_atts):\n",
    "        derivations.append(torch.argmax(sender_in[:, att * n_vals:(att + 1) * n_vals], dim=1))\n",
    "    derivations = torch.stack(derivations, dim=1)\n",
    "\n",
    "    protocol = {}\n",
    "    for i, derivation in enumerate(derivations):\n",
    "        protocol[tuple([torch.unsqueeze(elem, dim=0) for elem in derivation])] = k_hot_messages[i]\n",
    "\n",
    "    return protocol\n",
    "\n",
    "\n",
    "def get_name(atts, vals, vs, ml, seed):\n",
    "    name = ('atts' + str(atts) + '_vals' + str(vals) + '_vs' + str(vs) + '_len' + str(ml) + '/seed' +\n",
    "            str(seed) + '/')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeReconstructionError():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_concepts: int,\n",
    "            message_length: int,\n",
    "            vocab_size: int,\n",
    "            composition_fn: Type[CompositionFunction],\n",
    "            weight_decay=1e-5,\n",
    "    ):\n",
    "        self.num_concepts = num_concepts\n",
    "        self.message_length = message_length\n",
    "        self.composition_fn = composition_fn\n",
    "        self.weight_decay = weight_decay\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def measure(self, interaction) -> (float, float):\n",
    "        protocol = get_protocol(interaction, self.vocab_size)\n",
    "        objective = Objective(\n",
    "            num_concepts=self.num_concepts,\n",
    "            vocab_size=self.vocab_size,\n",
    "            message_length=self.message_length,\n",
    "            composition_fn=self.composition_fn(representation_size=self.message_length * self.vocab_size),\n",
    "            loss_fn=MultipleCrossEntropyLoss(representation_size=self.message_length * self.vocab_size,\n",
    "                                             message_length=self.message_length)\n",
    "        )\n",
    "        reconstruction_error_sum, reconstruction_error_mean = self._train_model(\n",
    "            messages=protocol.values(),\n",
    "            derivations=protocol.keys(),\n",
    "            objective=objective,\n",
    "            optimizer=torch.optim.Adam(objective.parameters(), lr=1e-1, weight_decay=self.weight_decay),\n",
    "            n_epochs=100\n",
    "        )\n",
    "        return reconstruction_error_sum, reconstruction_error_mean, objective\n",
    "\n",
    "    def evaluate(self, interaction, trained_objective) -> (float, float):\n",
    "        protocol = get_protocol(interaction, self.vocab_size)\n",
    "        messages = protocol.values()\n",
    "        derivations = protocol.keys()\n",
    "        errors = [trained_objective(message, derivation) for message, derivation in zip(messages, derivations)]\n",
    "        return sum(errors).item(), torch.mean(torch.tensor(errors)).item()\n",
    "\n",
    "    def _train_model(\n",
    "            self,\n",
    "            messages: Iterable[torch.Tensor],\n",
    "            derivations: Iterable[torch.Tensor],\n",
    "            objective: torch.nn.Module,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            n_epochs: int,\n",
    "            quiet: bool = False\n",
    "    ) -> (float, float):\n",
    "    \n",
    "        for t in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            errors = [objective(message, derivation) for message, derivation in zip(messages, derivations)]\n",
    "            loss = sum(errors)\n",
    "            loss.backward()\n",
    "            if not quiet and t % 100 == 0:\n",
    "                print(f'Training loss at epoch {t} is {loss.item():.4f}')\n",
    "            optimizer.step()\n",
    "        return loss.item(), torch.mean(torch.tensor(errors)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3 0\n",
      "objective tensor([[ 0.0503, -0.0570, -0.0528,  ...,  0.0019,  0.0137, -0.0262],\n",
      "        [ 0.0207,  0.0426, -0.0327,  ...,  0.0029,  0.0436,  0.0397],\n",
      "        [-0.0083, -0.0221, -0.0426,  ...,  0.0007, -0.0298, -0.0237],\n",
      "        ...,\n",
      "        [-0.0058,  0.0255, -0.0184,  ..., -0.0244,  0.0269,  0.0133],\n",
      "        [-0.0380,  0.0039, -0.0471,  ...,  0.0450, -0.0322, -0.0407],\n",
      "        [ 0.0395, -0.0501,  0.0085,  ...,  0.0369, -0.0131, -0.0194]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Training loss at epoch 0 is 2836.0022\n",
      "objective tensor([[-0.2969,  0.5314, -0.4386,  ..., -0.9910, -0.6670,  0.9044],\n",
      "        [ 0.5993,  0.7470, -0.5159,  ...,  0.5708, -0.5602,  0.5624],\n",
      "        [-0.6280,  0.5848,  0.4792,  ...,  0.4300,  0.5028,  0.6170],\n",
      "        ...,\n",
      "        [ 0.6203,  0.6096, -0.4140,  ..., -0.5592,  0.6453,  0.6325],\n",
      "        [-0.8003, -0.7029, -0.5203,  ...,  0.2983,  0.7802,  0.6779],\n",
      "        [-0.3941, -0.5870, -0.1276,  ...,  0.6752, -0.7277, -0.6584]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "train 0.0029661136213690042 1.2840314411732834e-05\n",
      "last tensor([[-0.2969,  0.5314, -0.4386,  ..., -0.9910, -0.6670,  0.9044],\n",
      "        [ 0.5993,  0.7470, -0.5159,  ...,  0.5708, -0.5602,  0.5624],\n",
      "        [-0.6280,  0.5848,  0.4792,  ...,  0.4300,  0.5028,  0.6170],\n",
      "        ...,\n",
      "        [ 0.6203,  0.6096, -0.4140,  ..., -0.5592,  0.6453,  0.6325],\n",
      "        [-0.8003, -0.7029, -0.5203,  ...,  0.2983,  0.7802,  0.6779],\n",
      "        [-0.3941, -0.5870, -0.1276,  ...,  0.6752, -0.7277, -0.6584]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "eval 1162.986328125 36.343326568603516\n",
      "eval 1697.381591796875 67.895263671875\n",
      "objective tensor([[ 0.0448, -0.0059,  0.0229,  ..., -0.0083, -0.0401, -0.0088],\n",
      "        [ 0.0567,  0.0192, -0.0355,  ...,  0.0222,  0.0138, -0.0457],\n",
      "        [-0.0042, -0.0331,  0.0090,  ...,  0.0217,  0.0056, -0.0306],\n",
      "        ...,\n",
      "        [ 0.0119, -0.0309, -0.0223,  ..., -0.0528,  0.0356,  0.0098],\n",
      "        [ 0.0072, -0.0520,  0.0142,  ...,  0.0153, -0.0292,  0.0225],\n",
      "        [-0.0390, -0.0201,  0.0060,  ...,  0.0139, -0.0346, -0.0393]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Training loss at epoch 0 is 2819.0962\n",
      "objective tensor([[ 0.5321,  0.6106, -0.4719,  ...,  0.5901,  0.6457,  0.9610],\n",
      "        [-0.1532, -0.9221, -0.4848,  ...,  0.5652,  0.6160,  0.5528],\n",
      "        [ 0.7982, -0.4041, -0.3111,  ...,  0.4824, -0.0035,  0.9520],\n",
      "        ...,\n",
      "        [ 0.7673, -0.8271, -0.4293,  ..., -0.4631,  0.6751,  0.5653],\n",
      "        [ 0.2364,  0.6927, -0.7365,  ..., -0.7777,  0.6413, -0.5206],\n",
      "        [-0.1882,  0.3459, -0.2873,  ..., -0.2561, -0.7834,  0.2770]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "train 0.0033245577942579985 1.4392022421816364e-05\n",
      "last tensor([[ 0.5321,  0.6106, -0.4719,  ...,  0.5901,  0.6457,  0.9610],\n",
      "        [-0.1532, -0.9221, -0.4848,  ...,  0.5652,  0.6160,  0.5528],\n",
      "        [ 0.7982, -0.4041, -0.3111,  ...,  0.4824, -0.0035,  0.9520],\n",
      "        ...,\n",
      "        [ 0.7673, -0.8271, -0.4293,  ..., -0.4631,  0.6751,  0.5653],\n",
      "        [ 0.2364,  0.6927, -0.7365,  ..., -0.7777,  0.6413, -0.5206],\n",
      "        [-0.1882,  0.3459, -0.2873,  ..., -0.2561, -0.7834,  0.2770]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "eval 1235.9608154296875 38.623779296875\n",
      "eval 1661.030517578125 66.44122314453125\n",
      "objective tensor([[-0.0400,  0.0326,  0.0168,  ...,  0.0028,  0.0473, -0.0440],\n",
      "        [ 0.0443,  0.0185, -0.0518,  ...,  0.0570, -0.0143,  0.0489],\n",
      "        [-0.0258,  0.0551, -0.0413,  ..., -0.0218,  0.0148,  0.0083],\n",
      "        ...,\n",
      "        [-0.0012,  0.0442,  0.0479,  ..., -0.0501, -0.0130, -0.0184],\n",
      "        [-0.0333, -0.0268, -0.0268,  ..., -0.0573,  0.0146,  0.0120],\n",
      "        [-0.0555,  0.0092, -0.0552,  ..., -0.0547, -0.0163, -0.0498]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Training loss at epoch 0 is 2823.9661\n",
      "objective tensor([[ 0.5480, -0.5264, -0.5857,  ...,  0.6188, -1.0151, -0.8016],\n",
      "        [-0.5549, -0.2404,  0.4634,  ..., -0.4682, -0.5870,  0.6349],\n",
      "        [ 0.6294, -0.3715, -0.8947,  ...,  0.5136, -0.6204, -0.1560],\n",
      "        ...,\n",
      "        [-0.8358, -0.6039,  0.7645,  ...,  0.5337,  0.4687, -0.6304],\n",
      "        [ 0.4408, -0.5758, -0.7817,  ...,  0.6873,  0.2368, -0.7557],\n",
      "        [ 0.0841, -0.6570, -0.3673,  ...,  0.3870,  0.5308,  0.7673]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "train 0.003091883147135377 1.338477432000218e-05\n",
      "last tensor([[ 0.5480, -0.5264, -0.5857,  ...,  0.6188, -1.0151, -0.8016],\n",
      "        [-0.5549, -0.2404,  0.4634,  ..., -0.4682, -0.5870,  0.6349],\n",
      "        [ 0.6294, -0.3715, -0.8947,  ...,  0.5136, -0.6204, -0.1560],\n",
      "        ...,\n",
      "        [-0.8358, -0.6039,  0.7645,  ...,  0.5337,  0.4687, -0.6304],\n",
      "        [ 0.4408, -0.5758, -0.7817,  ...,  0.6873,  0.2368, -0.7557],\n",
      "        [ 0.0841, -0.6570, -0.3673,  ...,  0.3870,  0.5308,  0.7673]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "eval 1213.4508056640625 37.92033767700195\n",
      "eval 1539.897705078125 61.595909118652344\n"
     ]
    }
   ],
   "source": [
    "n_atts = 2\n",
    "n_vals = 17\n",
    "prefix = 'C:/Users/Xenia/PycharmProjects/SystematicRepresentations/'\n",
    "    \n",
    "modes = ['test', 'generalization_hold_out', 'uniform_holdout']\n",
    "\n",
    "for message_length in [3]:\n",
    "    for vocab_size in [50]:\n",
    "        for seed_orig in range(1):\n",
    "            \n",
    "            print(vocab_size, message_length, seed_orig)\n",
    "            path = (prefix + 'egg/zoo/systematicity/results/' + \n",
    "                    get_name(n_atts, n_vals, vocab_size, message_length, seed_orig))\n",
    "\n",
    "            interaction_paths = {}\n",
    "            for mode in modes:\n",
    "                interaction_paths[mode] = path + 'interactions/' + mode + '/'\n",
    "            interactions = {}\n",
    "            for mode in modes:\n",
    "                for filename in os.listdir(interaction_paths[mode]):\n",
    "                    interactions[mode] = torch.load(interaction_paths[mode] + filename + '/interaction_gpu0')\n",
    "                    \n",
    "            NUM_SEEDS = 3\n",
    "            tre_errors = {}\n",
    "            for seed in range(NUM_SEEDS):\n",
    "                tre_errors['seed' + str(seed)] = {}\n",
    "                TRE = TreeReconstructionError(n_atts * n_vals, message_length, vocab_size, LinearComposition)\n",
    "                value_sum, value_mean, objective = TRE.measure(interactions['test'])\n",
    "                tre_errors['seed' + str(seed)]['_training_sum'] = value_sum\n",
    "                tre_errors['seed' + str(seed)]['_training_mean'] = value_mean\n",
    "                value_sum, value_mean = TRE.evaluate(interactions['generalization_hold_out'], objective)\n",
    "                tre_errors['seed' + str(seed)]['_generalization_holdout_sum'] = value_sum\n",
    "                tre_errors['seed' + str(seed)]['_generalization_holdout_mean'] = value_mean\n",
    "                value_sum, value_mean = TRE.evaluate(interactions['uniform_holdout'], objective)\n",
    "                tre_errors['seed' + str(seed)]['_uniform_holdout_sum'] = value_sum\n",
    "                tre_errors['seed' + str(seed)]['_uniform_holdout_mean'] = value_mean\n",
    "            \n",
    "            \n",
    "            pickle.dump(tre_errors, open(path + 'tre.pkl', 'wb'))\n",
    "            torch.save(objective, open(path + 'tre_objective.pt', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
